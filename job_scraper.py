# import os
# import time
# import random
# import pandas as pd
# from selenium import webdriver
# from selenium.webdriver.chrome.service import Service
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from selenium.common.exceptions import TimeoutException, NoSuchElementException
# from bs4 import BeautifulSoup
# from datetime import datetime
# from webdriver_manager.chrome import ChromeDriverManager
# import traceback


# class LinkedInJobScraper:
#     def __init__(self, headless=True, slow_mode=True):
#         """
#         Enhanced LinkedIn Job Scraper (no login required)
#         """
#         self.headless = headless
#         self.slow_mode = slow_mode
#         self.driver = None
#         self.jobs_data = []

#         # Rotate user agents to reduce detection
#         self.user_agents = [
#             'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#             'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15',
#             'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
#         ]

#     # -------------------- WebDriver Setup --------------------
#     def setup_driver(self):
#         """Set up Selenium WebDriver"""
#         chrome_options = Options()
#         if self.headless:
#             chrome_options.add_argument("--headless=new")
#         chrome_options.add_argument(f"user-agent={random.choice(self.user_agents)}")
#         chrome_options.add_argument("--no-sandbox")
#         chrome_options.add_argument("--disable-dev-shm-usage")
#         chrome_options.add_argument("--disable-blink-features=AutomationControlled")
#         chrome_options.add_argument("--disable-extensions")
#         chrome_options.add_argument("--window-size=1920,1080")

#         # Anti-detection tweaks
#         chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
#         chrome_options.add_experimental_option("useAutomationExtension", False)

#         service = Service(ChromeDriverManager().install())
#         driver = webdriver.Chrome(service=service, options=chrome_options)
#         driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
#             "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
#         })
#         self.driver = driver
#         return driver

#     def human_like_delay(self, min_seconds=1, max_seconds=3):
#         """Simulate human-like random delays"""
#         if self.slow_mode:
#             time.sleep(random.uniform(min_seconds, max_seconds))

#     # -------------------- Job Search --------------------
#     def search_jobs(self, keywords, location, limit=25, experience_level=None, job_type=None):
#         """
#         Scrape LinkedIn job listings without login.
#         Now collects detailed metadata and company info.
#         """
#         if self.driver is None:
#             self.setup_driver()

#         base_url = "https://www.linkedin.com/jobs/search/?"
#         params = {
#             "keywords": keywords.replace(" ", "%20"),
#             "location": location.replace(" ", "%20"),
#             "trk": "public_jobs_jobs-search-bar_search-submit",
#         }
#         url = base_url + "&".join([f"{key}={value}" for key, value in params.items()])

#         print(f"\nüîç Searching for '{keywords}' jobs in '{location}'")
#         print(f"URL: {url}\n")

#         try:
#             self.driver.get(url)
#             self.human_like_delay(3, 5)

#             job_urls = []
#             jobs_seen = set()
#             last_height = 0

#             # Scroll and collect job links
#             while len(job_urls) < limit:
#                 soup = BeautifulSoup(self.driver.page_source, 'html.parser')
#                 job_cards = soup.select("div.job-search-card")

#                 for job in job_cards:
#                     link_elem = job.select_one("a.base-card__full-link")
#                     if not link_elem:
#                         continue

#                     job_url = link_elem.get("href").split("?")[0]
#                     if job_url in jobs_seen:
#                         continue
#                     jobs_seen.add(job_url)

#                     title_elem = job.select_one("h3.base-search-card__title")
#                     company_elem = job.select_one("h4.base-search-card__subtitle")
#                     location_elem = job.select_one("span.job-search-card__location")
#                     time_elem = job.select_one("time")

#                     job_data = {
#                         "title": title_elem.text.strip() if title_elem else "N/A",
#                         "company": company_elem.text.strip() if company_elem else "N/A",
#                         "location": location_elem.text.strip() if location_elem else "N/A",
#                         "posted_time": time_elem.text.strip() if time_elem else "N/A",
#                         "job_url": job_url,
#                     }

#                     job_urls.append(job_data)
#                     if len(job_urls) >= limit:
#                         break

#                 self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
#                 self.human_like_delay(2, 4)
#                 new_height = self.driver.execute_script("return document.body.scrollHeight")
#                 if new_height == last_height:
#                     break
#                 last_height = new_height

#             print(f"üß≠ Found {len(job_urls)} job listings. Fetching detailed info...\n")

#             # Visit each job URL for full detail
#             for i, job in enumerate(job_urls[:limit], start=1):
#                 try:
#                     self.driver.get(job["job_url"])
#                     WebDriverWait(self.driver, 10).until(
#                         EC.presence_of_element_located((By.CSS_SELECTOR, "h1.topcard__title, h1.job-details-jobs-unified-top-card__job-title"))
#                     )
#                     self.human_like_delay(2, 4)

#                     details = self.extract_job_details(self.driver)
#                     job.update(details)

#                     self.jobs_data.append(job)
#                     print(f"‚úÖ [{i}/{limit}] {job['title']} at {job['company']}")

#                 except Exception as e:
#                     print(f"‚ö†Ô∏è Error fetching job details for {job.get('job_url')}: {e}")
#                     traceback.print_exc()
#                     continue

#             df = pd.DataFrame(self.jobs_data)
#             print(f"\n‚úÖ Successfully scraped {len(df)} detailed jobs.\n")
#             return df

#         except Exception as e:
#             print(f"‚ùå Error during scraping: {str(e)}")
#             traceback.print_exc()
#             return pd.DataFrame()

#         finally:
#             if self.driver:
#                 self.driver.quit()

#     # -------------------- Extract Job Details --------------------
#     def extract_job_details(self, driver):
#         """Extract detailed job and company info"""
#         soup = BeautifulSoup(driver.page_source, "html.parser")

#         # Job Description
#         desc_elem = soup.select_one(".show-more-less-html__markup")
#         description = desc_elem.get_text("\n", strip=True) if desc_elem else "N/A"

#         # Job Metadata
#         job_criteria = soup.select(".description__job-criteria-text")
#         criteria_labels = soup.select(".description__job-criteria-subheader")
#         details = {}
#         for label, value in zip(criteria_labels, job_criteria):
#             details[label.get_text(strip=True)] = value.get_text(strip=True)

#         # Company Info
#         company_elem = soup.select_one(".topcard__org-name-link, .job-details-jobs-unified-top-card__company-name a")
#         company_url = company_elem['href'] if company_elem and 'href' in company_elem.attrs else None

#         company_logo_elem = soup.select_one(".artdeco-entity-image")
#         company_logo = (
#             company_logo_elem.get("data-delayed-url") or company_logo_elem.get("src")
#             if company_logo_elem else None
#         )

#         company_about = ""
#         company_stats = []
#         if company_url:
#             company_about_data = self.scrape_company_about(company_url)
#             company_about = company_about_data.get("about", "")
#             company_stats = company_about_data.get("company_stats", [])

#         return {
#             "description": description,
#             "employment_type": details.get("Employment type", "N/A"),
#             "seniority_level": details.get("Seniority level", "N/A"),
#             "industries": details.get("Industries", "N/A"),
#             "job_function": details.get("Job function", "N/A"),
#             "company_url": company_url,
#             "company_logo": company_logo,
#             "company_about": company_about,
#             "company_stats": "; ".join(company_stats),
#         }

#     # -------------------- Company About Page --------------------
#     def scrape_company_about(self, company_url):
#         """Scrape company's About section"""
#         try:
#             if not company_url:
#                 return {}

#             if "linkedin.com" not in company_url:
#                 return {}

#             self.driver.get(company_url + "about/")
#             WebDriverWait(self.driver, 6).until(
#                 EC.presence_of_element_located((By.CSS_SELECTOR, ".org-top-card-summary-info-list__info-item"))
#             )
#             self.human_like_delay(2, 3)

#             soup = BeautifulSoup(self.driver.page_source, "html.parser")
#             about_text = soup.select_one(".core-section-container__content p")
#             about = about_text.get_text("\n", strip=True) if about_text else "N/A"

#             stats = [li.get_text(strip=True) for li in soup.select(".org-top-card-summary-info-list__info-item")]
#             return {"about": about, "company_stats": stats}

#         except Exception:
#             return {}

#     # -------------------- Save Methods --------------------
#     def save_to_csv(self, filename=None):
#         if not self.jobs_data:
#             return None
#         if not filename:
#             filename = f"linkedin_jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
#         pd.DataFrame(self.jobs_data).to_csv(filename, index=False)
#         return filename

#     def save_to_excel(self, filename=None):
#         if not self.jobs_data:
#             return None
#         if not filename:
#             filename = f"linkedin_jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
#         pd.DataFrame(self.jobs_data).to_excel(filename, index=False)
#         return filename

##############################################################################################
import os
import time
import random
import pandas as pd
import re
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
from datetime import datetime
from webdriver_manager.chrome import ChromeDriverManager
import traceback


class LinkedInJobScraper:
    def __init__(self, headless=True, slow_mode=True):
        """
        Enhanced LinkedIn Job Scraper with improved company data fetching
        """
        self.headless = headless
        self.slow_mode = slow_mode
        self.driver = None
        self.jobs_data = []
        self.company_cache = {}  # Cache company data to avoid re-scraping

        # Rotate user agents to reduce detection
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
        ]

    # -------------------- WebDriver Setup --------------------
    def setup_driver(self):
        """Set up Selenium WebDriver with enhanced anti-detection"""
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument("--headless=new")
        
        # Enhanced anti-detection options
        chrome_options.add_argument(f"user-agent={random.choice(self.user_agents)}")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-backgrounding-occluded-windows")
        chrome_options.add_argument("--disable-renderer-backgrounding")

        # Anti-detection tweaks
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option("useAutomationExtension", False)
        chrome_options.add_experimental_option("prefs", {
            "profile.default_content_setting_values.notifications": 2
        })

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # Additional anti-detection measures
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});
                window.chrome = {runtime: {}};
            """
        })
        
        self.driver = driver
        return driver

    def human_like_delay(self, min_seconds=1, max_seconds=3):
        """Simulate human-like random delays"""
        if self.slow_mode:
            time.sleep(random.uniform(min_seconds, max_seconds))

    def extract_skills(self, description):
        """Extract technical skills from job description"""
        if not description:
            return []
        
        # Common tech skills to look for
        tech_skills = [
            'Python', 'Java', 'JavaScript', 'TypeScript', 'React', 'Angular', 'Vue',
            'Node.js', 'Django', 'Flask', 'Spring', 'SQL', 'MongoDB', 'PostgreSQL',
            'AWS', 'Azure', 'Docker', 'Kubernetes', 'Git', 'Jenkins', 'CI/CD',
            'Machine Learning', 'Data Science', 'AI', 'TensorFlow', 'PyTorch',
            'HTML', 'CSS', 'REST API', 'GraphQL', 'Microservices', 'Agile', 'Scrum'
        ]
        
        found_skills = []
        description_lower = description.lower()
        
        for skill in tech_skills:
            if skill.lower() in description_lower:
                found_skills.append(skill)
        
        return found_skills

    def extract_salary(self, description):
        """Extract salary information from description"""
        if not description:
            return "N/A"
        
        # Look for salary patterns
        salary_patterns = [
            r'‚Çπ\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:-|to)\s*‚Çπ?\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(LPA|per\s*annum)',
            r'\$\s*(\d{1,3}(?:,\d{3})*)\s*(?:-|to)\s*\$?\s*(\d{1,3}(?:,\d{3})*)\s*(per\s*year|annually)',
            r'(\d{1,2})\s*(?:-|to)\s*(\d{1,2})\s*(LPA|lakhs)'
        ]
        
        for pattern in salary_patterns:
            match = re.search(pattern, description, re.IGNORECASE)
            if match:
                return f"{match.group(1)} - {match.group(2)} {match.group(3)}"
        
        return "N/A"

    def extract_applicant_count(self, soup):
        """Extract number of applicants"""
        try:
            applicant_selectors = [
                ".num-applicants__caption",
                ".jobs-unified-top-card__applicant-count"
            ]
            
            for selector in applicant_selectors:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    # Extract just the number
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        return f"{numbers[0]} applicants"
            
            return "N/A"
        except:
            return "N/A"

    def extract_company_info_from_job_page(self, soup):
        """Extract what company info we can from the job page itself"""
        company_info = {
            "about": "N/A",
            "company_stats": [],
            "industries": "N/A",
            "company_size": "N/A"
        }
        
        try:
            # Try to get company size from job criteria
            job_criteria = soup.select(".description__job-criteria-text")
            criteria_labels = soup.select(".description__job-criteria-subheader")
            
            for label, value in zip(criteria_labels, job_criteria):
                label_text = label.get_text(strip=True).lower()
                value_text = value.get_text(strip=True)
                
                if "industries" in label_text:
                    company_info["industries"] = value_text
                elif "company size" in label_text:
                    company_info["company_size"] = value_text
            
            # Try to extract company info from job description
            description_elem = soup.select_one(".show-more-less-html__markup")
            if description_elem:
                description = description_elem.get_text()
                
                # Look for company descriptions
                company_patterns = [
                    r'About (?:the )?company[:\-]?\s*(.{50,300})',
                    r'Company (?:overview|description)[:\-]?\s*(.{50,300})',
                    r'We are (?:a |an )?(.{50,300})',
                    r'Our company (.{50,300})'
                ]
                
                for pattern in company_patterns:
                    match = re.search(pattern, description, re.IGNORECASE | re.DOTALL)
                    if match:
                        potential_about = match.group(1).strip()
                        # Clean up the text
                        potential_about = re.sub(r'\s+', ' ', potential_about)
                        if len(potential_about) > 30:  # Only use if substantial
                            company_info["about"] = potential_about[:300] + "..." if len(potential_about) > 300 else potential_about
                            break
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting company info from job page: {e}")
        
        return company_info

    def scrape_company_about_enhanced(self, company_url):
        """Enhanced company about scraping with multiple strategies"""
        try:
            if not company_url or "linkedin.com" not in company_url:
                return {}
            
            # Check cache first
            if company_url in self.company_cache:
                print(f"üìã Using cached company data for {company_url}")
                return self.company_cache[company_url]
            
            print(f"üè¢ Attempting to fetch company data from {company_url}")
            
            # Strategy 1: Try the main company page first (not /about/)
            try:
                self.driver.get(company_url)
                self.human_like_delay(3, 5)
                
                soup = BeautifulSoup(self.driver.page_source, "html.parser")
                company_info = self.extract_company_info_from_main_page(soup)
                
                if company_info.get("about") and company_info["about"] != "N/A":
                    print("‚úÖ Got company data from main page")
                    self.company_cache[company_url] = company_info
                    return company_info
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Main page strategy failed: {e}")
            
            # Strategy 2: Try /about/ page with better selectors
            try:
                about_url = company_url.rstrip('/') + "/about/"
                self.driver.get(about_url)
                
                # Wait for different possible elements
                wait_selectors = [
                    ".org-top-card-summary-info-list__info-item",
                    ".org-about-us-organization-description",
                    ".break-words",
                    "[data-test-id='about-us-description']"
                ]
                
                element_found = False
                for selector in wait_selectors:
                    try:
                        WebDriverWait(self.driver, 3).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        element_found = True
                        break
                    except TimeoutException:
                        continue
                
                if element_found:
                    self.human_like_delay(2, 4)
                    soup = BeautifulSoup(self.driver.page_source, "html.parser")
                    company_info = self.extract_company_info_from_about_page(soup)
                    
                    if company_info.get("about") and company_info["about"] != "N/A":
                        print("‚úÖ Got company data from about page")
                        self.company_cache[company_url] = company_info
                        return company_info
                        
            except Exception as e:
                print(f"‚ö†Ô∏è About page strategy failed: {e}")
            
            # Strategy 3: Minimal company info from URL
            print("üìù Using minimal company info extraction")
            company_name = self.extract_company_name_from_url(company_url)
            minimal_info = {
                "about": f"Professional company profile for {company_name}. Visit LinkedIn for more details.",
                "company_stats": [],
                "industries": "N/A",
                "company_size": "N/A"
            }
            
            self.company_cache[company_url] = minimal_info
            return minimal_info
            
        except Exception as e:
            print(f"‚ùå All company scraping strategies failed: {e}")
            return {}

    def extract_company_info_from_main_page(self, soup):
        """Extract company info from main company page"""
        company_info = {
            "about": "N/A",
            "company_stats": [],
            "industries": "N/A",
            "company_size": "N/A"
        }
        
        try:
            # Try various selectors for company description
            about_selectors = [
                ".break-words p",
                ".org-top-card-summary__text",
                ".org-about-company-module__description",
                "[data-test-id='about-us-description']",
                ".org-top-card__description"
            ]
            
            for selector in about_selectors:
                about_elem = soup.select_one(selector)
                if about_elem:
                    about_text = about_elem.get_text(strip=True)
                    if len(about_text) > 30:  # Only use substantial descriptions
                        company_info["about"] = about_text[:500] + "..." if len(about_text) > 500 else about_text
                        break
            
            # Extract company stats
            stats_selectors = [
                ".org-top-card-summary-info-list__info-item",
                ".org-page-details__definition-text",
                ".company-industries"
            ]
            
            stats = []
            for selector in stats_selectors:
                stat_elements = soup.select(selector)
                for elem in stat_elements:
                    stat_text = elem.get_text(strip=True)
                    if stat_text and len(stat_text) < 100:  # Reasonable length
                        stats.append(stat_text)
            
            company_info["company_stats"] = stats[:5]  # Limit to 5 stats
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting from main page: {e}")
        
        return company_info

    def extract_company_info_from_about_page(self, soup):
        """Extract company info from about page with multiple selectors"""
        company_info = {
            "about": "N/A",
            "company_stats": [],
            "industries": "N/A",
            "company_size": "N/A"
        }
        
        try:
            # Enhanced selectors for about text
            about_selectors = [
                ".core-section-container__content p",
                ".org-about-us-organization-description__text",
                ".break-words",
                "[data-test-id='about-us-description']",
                ".artdeco-card .break-words p",
                ".org-about-company-module__description"
            ]
            
            for selector in about_selectors:
                about_elem = soup.select_one(selector)
                if about_elem:
                    about_text = about_elem.get_text("\n", strip=True)
                    if len(about_text) > 30:
                        company_info["about"] = about_text[:500] + "..." if len(about_text) > 500 else about_text
                        break
            
            # Enhanced selectors for company stats
            stats_selectors = [
                ".org-top-card-summary-info-list__info-item",
                ".org-about-company-module__company-staff-count-range",
                ".org-about-company-module__company-details dt + dd"
            ]
            
            stats = []
            for selector in stats_selectors:
                stat_elements = soup.select(selector)
                for elem in stat_elements:
                    stat_text = elem.get_text(strip=True)
                    if stat_text:
                        stats.append(stat_text)
            
            company_info["company_stats"] = stats[:5]
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting from about page: {e}")
        
        return company_info

    def extract_company_name_from_url(self, company_url):
        """Extract company name from LinkedIn URL"""
        try:
            # Extract from URL pattern like /company/company-name/
            match = re.search(r'/company/([^/]+)', company_url)
            if match:
                company_slug = match.group(1)
                # Convert slug to readable name
                return company_slug.replace('-', ' ').title()
        except:
            pass
        return "Company"

    # -------------------- Job Search --------------------
    def search_jobs(self, keywords, location, limit=25, experience_level=None, job_type=None):
        """
        Scrape LinkedIn job listings with enhanced company data fetching
        """
        if self.driver is None:
            self.setup_driver()

        base_url = "https://www.linkedin.com/jobs/search/?"
        params = {
            "keywords": keywords.replace(" ", "%20"),
            "location": location.replace(" ", "%20"),
            "trk": "public_jobs_jobs-search-bar_search-submit",
        }
        url = base_url + "&".join([f"{key}={value}" for key, value in params.items()])

        print(f"\nüîç Searching for '{keywords}' jobs in '{location}'")
        print(f"URL: {url}\n")

        try:
            self.driver.get(url)
            self.human_like_delay(3, 5)

            job_urls = []
            jobs_seen = set()
            last_height = 0

            # Scroll and collect job links with IMPROVED selectors
            while len(job_urls) < limit:
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # IMPROVED: Try multiple selectors for job cards
                job_cards = (soup.select("div.job-search-card") or 
                           soup.select("div.base-card") or 
                           soup.select("li.result-card"))

                for job in job_cards:
                    # IMPROVED: Try multiple selectors for job links
                    link_elem = (job.select_one("a.base-card__full-link") or
                               job.select_one("h3 a") or
                               job.select_one(".job-search-card__title a"))
                    
                    if not link_elem:
                        continue

                    job_url = link_elem.get("href").split("?")[0]
                    if job_url in jobs_seen:
                        continue
                    jobs_seen.add(job_url)

                    # IMPROVED: Try multiple selectors for job info
                    title_elem = (job.select_one("h3.base-search-card__title") or
                                job.select_one("h3") or 
                                job.select_one(".job-search-card__title"))
                    
                    company_elem = (job.select_one("h4.base-search-card__subtitle") or
                                  job.select_one("h4") or
                                  job.select_one(".job-search-card__subtitle"))
                    
                    location_elem = (job.select_one("span.job-search-card__location") or
                                   job.select_one(".base-search-card__location"))
                    
                    time_elem = job.select_one("time")

                    job_data = {
                        "title": title_elem.text.strip() if title_elem else "N/A",
                        "company": company_elem.text.strip() if company_elem else "N/A",
                        "location": location_elem.text.strip() if location_elem else "N/A",
                        "posted_time": time_elem.text.strip() if time_elem else "N/A",
                        "job_url": job_url,
                    }

                    job_urls.append(job_data)
                    if len(job_urls) >= limit:
                        break

                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                self.human_like_delay(2, 4)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

            print(f"üß≠ Found {len(job_urls)} job listings. Fetching detailed info...\n")

            # Visit each job URL for full detail
            for i, job in enumerate(job_urls[:limit], start=1):
                try:
                    self.driver.get(job["job_url"])
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "h1.topcard__title, h1.job-details-jobs-unified-top-card__job-title"))
                    )
                    self.human_like_delay(2, 4)

                    details = self.extract_job_details(self.driver)
                    job.update(details)

                    self.jobs_data.append(job)
                    print(f"‚úÖ [{i}/{limit}] {job['title']} at {job['company']}")
                    
                    # Add a longer delay every 5 jobs to avoid rate limiting
                    if i % 5 == 0:
                        print("üïí Taking a breather to avoid rate limits...")
                        self.human_like_delay(5, 8)

                except Exception as e:
                    print(f"‚ö†Ô∏è Error fetching job details for {job.get('job_url')}: {e}")
                    continue

            df = pd.DataFrame(self.jobs_data)
            print(f"\n‚úÖ Successfully scraped {len(df)} detailed jobs.\n")
            
            # Print company data success rate
            jobs_with_company_info = len([j for j in self.jobs_data if j.get('company_about') and j['company_about'] != 'N/A'])
            print(f"üìä Company data success rate: {jobs_with_company_info}/{len(df)} ({(jobs_with_company_info/len(df)*100):.1f}%)")
            
            return df

        except Exception as e:
            print(f"‚ùå Error during scraping: {str(e)}")
            traceback.print_exc()
            return pd.DataFrame()

        finally:
            if self.driver:
                self.driver.quit()

    # -------------------- Extract Job Details --------------------
    def extract_job_details(self, driver):
        """Extract detailed job and company info with IMPROVED data extraction and enhanced company fetching"""
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # IMPROVED: Job Description with multiple selectors
        desc_selectors = [
            ".show-more-less-html__markup",
            ".jobs-description-content__text", 
            ".job-details-jobs-unified-top-card__job-description"
        ]
        
        description = "N/A"
        for selector in desc_selectors:
            desc_elem = soup.select_one(selector)
            if desc_elem:
                description = desc_elem.get_text("\n", strip=True)
                break

        # Job Metadata
        job_criteria = soup.select(".description__job-criteria-text")
        criteria_labels = soup.select(".description__job-criteria-subheader")
        details = {}
        for label, value in zip(criteria_labels, job_criteria):
            details[label.get_text(strip=True)] = value.get_text(strip=True)

        # Company Info - Enhanced extraction
        company_elem = soup.select_one(".topcard__org-name-link, .job-details-jobs-unified-top-card__company-name a")
        company_url = company_elem['href'] if company_elem and 'href' in company_elem.attrs else None

        company_logo_elem = soup.select_one(".artdeco-entity-image")
        company_logo = (
            company_logo_elem.get("data-delayed-url") or company_logo_elem.get("src")
            if company_logo_elem else None
        )

        # Enhanced company data fetching
        company_about = "N/A"
        company_stats = []
        
        # First, try to get company info from the job page itself
        job_page_company_info = self.extract_company_info_from_job_page(soup)
        if job_page_company_info.get("about") != "N/A":
            company_about = job_page_company_info["about"]
            company_stats = job_page_company_info.get("company_stats", [])
        
        # Then, try to enhance with company page data (if we have a URL and didn't get good info from job page)
        if company_url and (company_about == "N/A" or len(company_about) < 50):
            try:
                company_about_data = self.scrape_company_about_enhanced(company_url)
                if company_about_data.get("about") and company_about_data["about"] != "N/A":
                    company_about = company_about_data["about"]
                    company_stats = company_about_data.get("company_stats", [])
            except Exception as e:
                print(f"‚ö†Ô∏è Company page scraping failed, using job page data: {e}")

        # IMPROVED: Extract additional data
        skills = self.extract_skills(description)
        salary_info = self.extract_salary(description)
        applicant_count = self.extract_applicant_count(soup)

        return {
            "description": description,
            "employment_type": details.get("Employment type", "N/A"),
            "seniority_level": details.get("Seniority level", "N/A"),
            "industries": details.get("Industries", "N/A"),
            "job_function": details.get("Job function", "N/A"),
            "company_url": company_url,
            "company_logo": company_logo,
            "company_about": company_about,
            "company_stats": "; ".join(company_stats) if company_stats else "N/A",
            # NEW IMPROVED FIELDS:
            "extracted_skills": ", ".join(skills) if skills else "N/A",
            "salary_range": salary_info,
            "applicant_count": applicant_count,
        }

    # -------------------- Save Methods --------------------
    def save_to_csv(self, filename=None):
        if not self.jobs_data:
            return None
        if not filename:
            filename = f"linkedin_jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        pd.DataFrame(self.jobs_data).to_csv(filename, index=False)
        return filename

    def save_to_excel(self, filename=None):
        if not self.jobs_data:
            return None
        if not filename:
            filename = f"linkedin_jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        pd.DataFrame(self.jobs_data).to_excel(filename, index=False)
        return filename


# Test the improved scraper
if __name__ == "__main__":
    scraper = LinkedInJobScraper(headless=False, slow_mode=True)
    
    try:
        df = scraper.search_jobs(
            keywords="Python Developer",
            location="Bangalore, India",
            limit=5
        )
        
        if not df.empty:
            print(f"\n‚úÖ Successfully scraped {len(df)} jobs!")
            
            # Show enhanced data
            jobs_with_skills = df[df['extracted_skills'] != 'N/A']
            jobs_with_salary = df[df['salary_range'] != 'N/A']
            jobs_with_company_info = df[df['company_about'] != 'N/A']
            
            print(f"\nüìä IMPROVED Data Quality:")
            print(f"  - Jobs with skills: {len(jobs_with_skills)}/{len(df)}")
            print(f"  - Jobs with salary: {len(jobs_with_salary)}/{len(df)}")
            print(f"  - Jobs with company info: {len(jobs_with_company_info)}/{len(df)}")
            
            # Save files
            csv_file = scraper.save_to_csv()
            excel_file = scraper.save_to_excel()
            print(f"\nüíæ Files saved: {csv_file}, {excel_file}")
            
        else:
            print("‚ùå No jobs found")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
# ```

# The key improvements in this new version are:

# 1. **Different scraping approach**: Instead of trying to click on job cards (which seems to be failing), the new approach:
#    - First extracts the direct URL for each job listing from the search results page
#    - Then visits each job URL directly to scrape the details

# 2. **Simplified selectors**: Focused on the specific selectors that were detected in your debug output (it found 60 job cards and 60 job titles)

# 3. **More robust error handling**: Better tracking of what's being extracted and what's failing

# Try replacing your `linkedin_job_scraper.py` file with this updated code and run it again with:
# ```
# python app.py -k "Python Developer" -l "Remote" --headless False